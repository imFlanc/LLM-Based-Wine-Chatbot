from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import logging
import torch
import time
import warnings
import os
import sys
import contextlib

# âœ… ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")
logging.set_verbosity_error()

# âœ… í‘œì¤€ ì¶œë ¥ ìˆ¨ê¸°ê¸° ë„ìš°ë¯¸
@contextlib.contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout

# âœ… ëª¨ë¸ ID
model_id = "mistralai/Mistral-7B-Instruct-v0.1"
HF_TOKEN = "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"  # Hugging Faceì—ì„œ ë°œê¸‰í•œ í† í° ë„£ê¸°

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Wine Expert Persona)
def build_prompt(user_input: str) -> str:
    return f"""<s>[INST]
You are a knowledgeable, trustworthy wine expert. You explain wine-related knowledge clearly and accurately, suitable for both complete beginners and those studying for wine certifications.

When responding:
- Be honest and do not hallucinate information. If unsure, say so.
- Use plain but professional language. Avoid jargon unless necessary, and explain it when used.
- Provide helpful, actionable knowledge that the user can understand or remember.
- If the topic is advanced, briefly mention that it's more complex and offer a simple explanation.

Question: {user_input}
[/INST]
"""

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    use_auth_token=HF_TOKEN
)

# âœ… í˜„ì¬ ë””ë°”ì´ìŠ¤ ì¶œë ¥
print("ğŸ§  ëª¨ë¸ ë¡œë“œëœ ë””ë°”ì´ìŠ¤:", model.device)
print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ğŸ¯ í˜„ì¬ GPU:", torch.cuda.get_device_name(torch.cuda.current_device()))

print("ğŸ’¬ Mistral LLM Chat ì‹œì‘! ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)\n")

# âœ… ì±„íŒ… ë£¨í”„
while True:
    user_input = input("You: ")
    if user_input.strip().lower() in ['exit', 'quit']:
        break

    prompt = build_prompt(user_input)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    print("ğŸ¤– ëª¨ë¸ì´ ì¶”ë¡  ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n")

    with suppress_stdout():
        outputs = model.generate(
            **inputs,
            max_new_tokens=64,
            do_sample=False,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1
        )

    # âœ… í”„ë¡¬í”„íŠ¸ ì´í›„ í† í°ë§Œ ì¶”ì¶œ
    output_tokens = outputs[0][len(inputs["input_ids"][0]):]
    token_list = tokenizer.convert_ids_to_tokens(output_tokens)
    response = tokenizer.convert_tokens_to_string(token_list).strip()

    # âœ… ëŠê¸´ ë¬¸ì¥ ë³´ì™„
    if not response.endswith(('.', '!', '?')):
        response += "..."

    # âœ… ChatGPT ìŠ¤íƒ€ì¼ ì¶œë ¥
    print("LLM: ", end="", flush=True)
    for char in response:
        print(char, end="", flush=True)
        time.sleep(0.015)
    print("\n" + "-" * 60)
