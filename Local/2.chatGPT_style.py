# chat_mistral.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
from chatbot import HF_TOKEN, model_id

# âœ… ëª¨ë¸ ID
model_id = "mistralai/Mistral-7B-Instruct-v0.1"

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Wine Expert Persona)
def build_prompt(user_input: str) -> str:
    return f"""<s>[INST]
You are a knowledgeable, trustworthy wine expert. You explain wine-related knowledge clearly and accurately, suitable for both complete beginners and those studying for wine certifications.

When responding:
- Be honest and do not hallucinate information. If unsure, say so.
- Use plain but professional language. Avoid jargon unless necessary, and explain it when used.
- Provide helpful, actionable knowledge that the user can understand or remember.
- If the topic is advanced, briefly mention that it's more complex and offer a simple explanation.

Question: {user_input}
[/INST]
"""

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",         # GPU ìë™ ì„¤ì •
    torch_dtype=torch.float16, # 4070Tiì— ìµœì 
    token=HF_TOKEN    # ë¡œê·¸ì¸ ì¸ì¦ í† í° ì‚¬ìš©
)

# âœ… ì±„íŒ… ë£¨í”„ ì‹œì‘
print("ğŸ’¬ Mistral LLM Chat ì‹œì‘! ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)")

while True:
    user_input = input("You: ")
    if user_input.lower() in ['exit', 'quit']:
        break

    prompt = build_prompt(user_input)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    print("ğŸ¤– ëª¨ë¸ì´ ì¶”ë¡  ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=False,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )

    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # í”„ë¡¬í”„íŠ¸ ì œê±°
    if prompt in full_output:
        response = full_output.replace(prompt, "").strip()
    else:
        response = full_output.strip()

    # âœ… í† í° ë‹¨ìœ„ë¡œ ì¶œë ¥ (ChatGPT ìŠ¤íƒ€ì¼)
    print("LLM: ", end="", flush=True)
    for token_id in outputs[0][len(inputs["input_ids"][0]):]:  # ì…ë ¥ ì´í›„ì˜ í† í°ë§Œ ì¶œë ¥
        token_str = tokenizer.decode(token_id, skip_special_tokens=True)
        print(token_str, end="", flush=True)
        time.sleep(0.015)  # ë„ˆë¬´ ë¹ ë¥´ë©´ 0.01~0.02 ì‚¬ì´ë¡œ ì¡°ì •
    print("\n" + "-" * 60)
