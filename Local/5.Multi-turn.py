# Contextë¥¼ ëª‡ í„´ì´ë¼ë„ í•  ìˆ˜ ìˆë„ë¡ multi-turnìœ¼ë¡œ ë³€ê²½
# Mistralì€ ì• ì´ˆì— í•œ í„´ QAì— ìµœì í™”ëœ ëª¨ë¸ì´ë‹¤? ê²°ë¡ : ì§€ê¸ˆ ìƒíƒœë¡œ ì•ˆ ë¨.

from colorama import init, Fore, Style
import os
import sys
import time
import torch
import warnings
import contextlib
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import logging
from chatbot import HF_TOKEN, model_id

# âœ… ì´ˆê¸°í™”: ì»¬ëŸ¬, ê²½ê³ , ë¡œê¹…
init(autoreset=True)
warnings.filterwarnings("ignore")
logging.set_verbosity_error()

# âœ… í‘œì¤€ ì¶œë ¥ ìˆ¨ê¸°ê¸° ë„ìš°ë¯¸
@contextlib.contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout

# âœ… ëª¨ë¸ ID
model_id = "mistralai/Mistral-7B-Instruct-v0.1"

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Wine Expert Persona)
def build_prompt(context: list, user_input: str) -> str:
    prompt = f"""<s>[INST]
    You are a knowledgeable, trustworthy wine expert. You explain wine-related knowledge clearly and accurately, suitable for both complete beginners and those studying for wine certifications.

    When responding:
    - Be honest and do not hallucinate information. If unsure, say so.
    - Use plain but professional language. Avoid jargon unless necessary, and explain it when used.
    - Provide helpful, actionable knowledge that the user can understand or remember.
    - If the topic is advanced, briefly mention that it's more complex and offer a simple explanation.

    Question: {user_input}
    [/INST]
    """
    return prompt

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    token=HF_TOKEN
)

# âœ… í˜„ì¬ ë””ë°”ì´ìŠ¤ ì¶œë ¥
print("ğŸ§  ëª¨ë¸ ë¡œë“œëœ ë””ë°”ì´ìŠ¤:", model.device)
print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ğŸ¯ í˜„ì¬ GPU:", torch.cuda.get_device_name(torch.cuda.current_device()))

print("ğŸ’¬ Mistral LLM Chat ì‹œì‘! ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)\n")

# âœ… ëŒ€í™” ë£¨í”„
chat_history = []
while True:
    user_input = input(Fore.GREEN + "You: " + Style.RESET_ALL)
    if user_input.strip().lower() in ['exit', 'quit']:
        break

    prompt = build_prompt(chat_history, user_input)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    print("ğŸ¤– ëª¨ë¸ì´ ì¶”ë¡  ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n")

    with suppress_stdout():
        outputs = model.generate(
            **inputs,
            max_new_tokens=16,
            do_sample=False,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1
        )

    # âœ… í”„ë¡¬í”„íŠ¸ ì´í›„ í† í°ë§Œ ì¶”ì¶œ
    output_tokens = outputs[0][len(inputs["input_ids"][0]):]
    token_list = tokenizer.convert_ids_to_tokens(output_tokens)
    response = tokenizer.convert_tokens_to_string(token_list).strip()

    # âœ… ëŠê¸´ ë¬¸ì¥ ë³´ì™„
    if not response.endswith(('.', '!', '?')):
        response += "..."

    # âœ… ChatGPT ìŠ¤íƒ€ì¼ ì¶œë ¥
    print(Fore.BLUE + "LLM: " + Style.RESET_ALL, end="", flush=True)
    for char in response:
        print(char, end="", flush=True)
        time.sleep(0.015)
    print("\n" + "-" * 60)
    
    # ëŒ€í™” ì´ë ¥ ì €ì¥ (ì‚¬ìš©ì ì§ˆë¬¸ + ì‘ë‹µ)
    chat_history.append(f"[INST] {user_input} [/INST] {response}")