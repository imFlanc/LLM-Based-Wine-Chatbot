# Memoryë¥¼ Bufferì—ì„œ Summaryë¡œ êµì²´í•¨.
# í˜¼í•© ë°©ì‹ë„ ê³ ë¯¼. ê·¼ë° ì–´ì°¨í”¼ ì²˜ìŒì— promptë¡œ ì‹œì‘í•˜ëŠ”ë° êµ³ì´?

from colorama import init, Fore, Style
import os
import sys
import time
import torch
import warnings
import contextlib
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from transformers.utils import logging

# âœ… LangChain ê´€ë ¨ import
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryMemory # BufferMemoryëŠ” ì´ì „ ëŒ€í™”ë¥¼ ëª¨ë‘ ëˆ„ì í•´ì„œ í† í° í­ë°œí•¨
from langchain.prompts import PromptTemplate

from chatbot import HF_TOKEN, model_id

# âœ… ì´ˆê¸°í™”
init(autoreset=True)
warnings.filterwarnings("ignore")
logging.set_verbosity_error()

@contextlib.contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout

# âœ… ëª¨ë¸ ID
model_id = "mistralai/Mistral-7B-Instruct-v0.1"

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    token=HF_TOKEN
)

print("ğŸ§  ëª¨ë¸ ë¡œë“œëœ ë””ë°”ì´ìŠ¤:", model.device)
print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ğŸ¯ í˜„ì¬ GPU:", torch.cuda.get_device_name(torch.cuda.current_device()))

# âœ… í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=32,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1,
)

# âœ… LangChain LLM ë˜í¼
llm = HuggingFacePipeline(pipeline=generator)

# âœ… ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ (INST êµ¬ì¡° í¬í•¨)
custom_template = """<s>[INST]
You are a knowledgeable, trustworthy wine expert. You explain wine-related knowledge clearly and accurately, suitable for both complete beginners and those studying for wine certifications.

When responding:
- Be honest and do not hallucinate information. If unsure, say so.
- Use plain but professional language. Avoid jargon unless necessary, and explain it when used.
- Provide helpful, actionable knowledge that the user can understand or remember.
- If the topic is advanced, briefly mention that it's more complex and offer a simple explanation.

Conversation so far:
{history}
Human: {input}
[/INST]
Wine Expert:"""

prompt = PromptTemplate(
    input_variables=["history", "input"],
    template=custom_template
)

# âœ… ëŒ€í™” ìš”ì•½! ë©”ëª¨ë¦¬ + ì²´ì¸ êµ¬ì„±
memory = ConversationSummaryMemory(
    llm=llm,                   # ìš”ì•½ì— ì‚¬ìš©í•  ëª¨ë¸
    return_messages=False,     # Trueë©´ chat formatìœ¼ë¡œ ì €ì¥ë¨
    memory_key="history"       # promptì—ì„œ ì‚¬ìš©ëœ ë³€ìˆ˜ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨
)
chat_chain = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=False)

# âœ… ëŒ€í™” ë£¨í”„
print("ğŸ’¬ ì™€ì¸ ì „ë¬¸ê°€ LLMê³¼ì˜ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)\n")

while True:
    user_input = input(Fore.GREEN + "You: " + Style.RESET_ALL)
    if user_input.strip().lower() in ['exit', 'quit']:
        break

    # print("ğŸ¤– ëª¨ë¸ì´ ì¶”ë¡  ì¤‘ì…ë‹ˆë‹¤...\n")
    with suppress_stdout():
        # LangChain run ê²°ê³¼
        raw_response = chat_chain.run(user_input)

    # âœ… Wine Expert ì´í›„ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if "Wine Expert:" in raw_response:
        response = raw_response.split("Wine Expert:")[-1].strip()
    else:
        response = raw_response.strip()

    # âœ… ëŠê¸´ ë¬¸ì¥ ë³´ì™„
    if not response.endswith(('.', '!', '?')):
        response += "..."

    # âœ… ì¶œë ¥ (ChatGPT ìŠ¤íƒ€ì¼)
    print(Fore.BLUE + "LLM: " + Style.RESET_ALL, end="", flush=True)
    for char in response:
        print(char, end="", flush=True)
        time.sleep(0.015)
    print("\n" + "-" * 60)    